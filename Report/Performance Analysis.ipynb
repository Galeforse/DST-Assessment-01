{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Hamming distance to analyse our models. The hamming distance of two binary arrays $ x $ and $ y $ is the number of positions at which the corresponding symbols are different: \n",
    "\n",
    "$$H(X,Y) = \\sum\\limits_{i=1}^n \\mathbb{1}(x_i = y_i)$$\n",
    "\n",
    "We decided to use this metric as it is uniform across the models and gives insight into accuracy of the model. Some of the models predicted the attack type whereas others predicted normal vs non-normal behaviour which means we had to find a standardised test across the two types of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matt - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hamming Distance</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Matt</td>\n",
       "      <td>3</td>\n",
       "      <td>99.993928%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hamming Distance    Accuracy\n",
       "Matt                 3  99.993928%"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_matt = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/test_predictions_matt.csv')\n",
    "test_labels = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/test_labels.csv')\n",
    "\n",
    "pred_matt = np.array(pred_matt['0'])\n",
    "test_labels = np.array(test_labels['label'])\n",
    "\n",
    "hamm_dist_matt = distance.hamming(pred_matt,test_labels)\n",
    "accuracy_matt = 1 - hamm_dist_matt\n",
    "hamm_dist_matt = int(hamm_dist_matt * len(test_labels))\n",
    "\n",
    "pd.DataFrame([[hamm_dist_matt, \"{:.6%}\".format(accuracy_matt)]], columns = ['Hamming Distance', 'Accuracy'], index = ['Matt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alex - KNN (K Nearest Neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hamming Distance</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>886</td>\n",
       "      <td>98.20%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hamming Distance Accuracy\n",
       "0               886   98.20%"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' R code to get the Hamming distance for Alex's model's result'''\n",
    "## library(class)\n",
    "## library(caret)\n",
    "\n",
    "# pr1 <- knn(KTT_train,KTT_test,cl=KTT_target_category,k=1, use.all=FALSE)\n",
    "## [...]\n",
    "## [...]\n",
    "# pr37 <- knn(KTT_train,KTT_test,cl=KTT_target_category,k=1, use.all=FALSE)\n",
    "\n",
    "# h<- vector(length=37)\n",
    "# h[1]<-hamming.distance(as.vector(pr1), kt$Behaviour[1:1333])\n",
    "## [...]\n",
    "## [...]\n",
    "# h[37]<-hamming.distance(as.vector(pr37), kt$Behaviour[46790:48122])\n",
    "\n",
    "# m <- sum(h) ## The total Hamming Distance\n",
    "\n",
    "# h <- -h\n",
    "# h<- h+1333\n",
    "# h<- h/1333\n",
    "# summary(h) # mean = 98.2% accuracy. ## The accuracy\n",
    "\n",
    "pred_alex_df = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/KNN-Performance.csv')\n",
    "\n",
    "hamm_dist_alex = pred_alex_df.iat[0,0]\n",
    "accuracy_alex = pred_alex_df.iat[0,1]\n",
    "\n",
    "pred_alex_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luke - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hamming Distance</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Luke</td>\n",
       "      <td>1391</td>\n",
       "      <td>97.184325%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hamming Distance    Accuracy\n",
       "Luke              1391  97.184325%"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_luke = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/Y_pred_Luke.csv')\n",
    "test_labels = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/Y_test_Luke.csv')\n",
    "\n",
    "pred_luke = np.array(pred_luke['0'])\n",
    "test_labels = np.array(test_labels['label'])\n",
    "\n",
    "hamm_dist_luke = distance.hamming(pred_luke,test_labels)\n",
    "accuracy_luke = 1 - hamm_dist_luke\n",
    "hamm_dist_luke = int(hamm_dist_luke * len(test_labels))\n",
    "\n",
    "pd.DataFrame([[hamm_dist_luke, \"{:.6%}\".format(accuracy_luke)]], columns = ['Hamming Distance', 'Accuracy'], index = ['Luke'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabriel - Logistice Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have two trained models, one using basic Logistic Regression and the other using `GridSearchCV` with cross-validation. Despite the Grid version taking almost 80 times as long to train as the basic model, the actual increase in accuracy was rather small, especially comparing the hamming distance of the two, with that many data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hamming Distance</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Gabe Grid</td>\n",
       "      <td>73</td>\n",
       "      <td>99.852233%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gabe Basic</td>\n",
       "      <td>76</td>\n",
       "      <td>99.846160%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hamming Distance    Accuracy\n",
       "Gabe Grid                 73  99.852233%\n",
       "Gabe Basic                76  99.846160%"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_gabe_grid = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/grid_y_pred_gabe.csv')\n",
    "pred_gabe_basic = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/reg_y_pred_gabe.csv')\n",
    "test_labels = pd.read_csv('https://github.com/Galeforse/DST-Assessment-01/raw/main/Data/y_test_gabe.csv')\n",
    "\n",
    "pred_gabe_grid = np.array(pred_gabe_grid['0'])\n",
    "pred_gabe_basic = np.array(pred_gabe_basic['0'])\n",
    "test_labels = np.array(test_labels['0'])\n",
    "\n",
    "hamm_dist_gabe = distance.hamming(pred_gabe_grid,test_labels)\n",
    "accuracy_gabe = 1 - hamm_dist_gabe\n",
    "hamm_dist_gabe = int(hamm_dist_gabe * len(test_labels))\n",
    "\n",
    "hamm_dist_gabe2 = distance.hamming(pred_gabe_basic,test_labels)\n",
    "accuracy_gabe2 = 1 - hamm_dist_gabe2\n",
    "hamm_dist_gabe2 = int(hamm_dist_gabe2 * len(test_labels))\n",
    "\n",
    "pd.DataFrame([[hamm_dist_gabe, \"{:.6%}\".format(accuracy_gabe)],[hamm_dist_gabe2, \"{:.6%}\".format(accuracy_gabe2)]], columns = ['Hamming Distance', 'Accuracy'], index = ['Gabe Grid','Gabe Basic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hamming Distance</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Matt</td>\n",
       "      <td>3</td>\n",
       "      <td>99.993928%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Alex</td>\n",
       "      <td>886</td>\n",
       "      <td>98.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Luke</td>\n",
       "      <td>1391</td>\n",
       "      <td>97.184325%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Gabriel</td>\n",
       "      <td>73</td>\n",
       "      <td>99.852233%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hamming Distance    Accuracy\n",
       "Matt                    3  99.993928%\n",
       "Alex                  886      98.20%\n",
       "Luke                 1391  97.184325%\n",
       "Gabriel                73  99.852233%"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[hamm_dist_matt, \"{:.6%}\".format(accuracy_matt)],[hamm_dist_alex, accuracy_alex],\n",
    "              [hamm_dist_luke, \"{:.6%}\".format(accuracy_luke)],[hamm_dist_gabe, \"{:.6%}\".format(accuracy_gabe)]],\n",
    "             columns = ['Hamming Distance', 'Accuracy'], index = ['Matt', 'Alex', 'Luke', 'Gabriel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This report reflects the results we obtained from running our models. Matt ran a random forest model which had an accuracy of $99.99\\%$, Alex ran a k-NN model with k=1 and had an accuracy of $98.20\\%$, Luke ran a Bayesian model which had an accuracy of $97.18\\%$ and Gabriel ran a Logistic Regression model and obtain a $99.85\\%$ accuracy. Thus we have a scoring of:\n",
    "\n",
    "1. Random Forest\n",
    "2. Logistic Regression\n",
    "3. K-NN \n",
    "4. Gaussian Naive Bayes\n",
    "\n",
    "All of the models give very accurate results of $>97\\%$. The most accurate model is the random forest for this data and produces the most accurate results. This may be due to the ability of a random forest to handle all the features we have and not having to work on a reduced space which the k-nn model had to. It makes sense that the Gaussian Naive Bayes was the least successful; as it's name suggests it makes naive assumptions. However all the models were very accurate overal, all being within a 5% of the actual data; quite significant!\n",
    "\n",
    "The first issue that we faced in this project was what code set to use. This was an issue as we all wanted to make sure that we tested on the same set of data. If we were all using the same coding language we could have all parsed the same code over the data to split it into testing and training data within our individual reports by settign the same seed, however we used a mix of R and Python in the group and therefore we had to make sure we split the code beforehand (as detailed in the introduction section).\n",
    "\n",
    "A great deal of understanding had to be made by each member of the group to successfully implement their model. A combination of general research on the internet, and reading documentation of functions were the most important factors in helping each of us understand our models better, and we communicated to each other the basics of each model, to aid our personal understanding of available models; perhaps for use in future endeavours. As mentioned earlier, we decided to use the hamming distance as a measure of accuracy as it is uniformly easy to implement across all models, however it was not just the hamming distance that could be shared successfully. As a group we communicated several useful sections of code, and methods for others to use in order to improve the functioning of each model, and improve general code performance/clarity.\n",
    "\n",
    "The most difficult process for all of our models was the data processing to allow the models to run on the data. This involved:\n",
    "\n",
    "- Re-labelling the data to 0's and 1's to allow us to model normal vs non-normal data since many models are binary.\n",
    "- normalising data.\n",
    "- creating dummy variables to allow us to run models on categorical features.\n",
    "\n",
    "There was also a variety of other issues that arose during the coding process. Notably, training some of the models took a very long time and depending on the current coding environment could fail for a number of reasons. This prompted different actions needing to be taken in order to make sure the model could complete successfully. Using the web-based version of Jupyter Notebook did not work for everyone as the amount of data stored in the RAM when training the data was sometimes so large it would exceed the limit and would therefore fail, and additionally the kernel would sometimes die if left alone for too long, causing the script to stop running.\n",
    "\n",
    "In the KNN test, memory problems were solved by splitting the data into several chunks and combing them all together. This could have potentially affected the accuracy of the results for this test, however as mentioned above, the test was still rather accurate and if it did suffer from this, it was nothing major.\n",
    "\n",
    "After the data issues were resolved, the models were run on our training data to develop them and allow them to learn about the differences between non-normal and normal data in comparison to other variables in the data, and then tested on the 10% test data we created. Further tests were conducted by some of us with testing on external text data such as the extra KD99 test data and also testing on non-binary data; as well as additional tests using slightly different methods, submitting our best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
